{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vithu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:\\yamuna\\practise_file\\fine_tuned_recipe_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe2(prompt, max_length=300):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],  # âœ… Fixes the warning\n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7, \n",
    "            top_k=50, \n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.eos_token_id  # âœ… Explicitly set pad token\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vithu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vithu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Title: Pasta\\nServings: 2\\n\"\n",
    "generated_recipe = generate_recipe(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Recipe:\n",
      " Title: Pasta\n",
      "Servings: 2\n",
      "Ingredients: [\"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground lamb\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"1 lb. ground pork\", \"1 lb. ground turkey\", \"1 lb. ground beef\", \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated Recipe:\\n\", generated_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = r\"D:\\yamuna\\practise_file\\fine_tuned_recipe_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Ensure padding is set correctly\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate a recipe\n",
    "def generate_recipe1(prompt, max_length=300):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],  \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            temperature=0.9,  # ðŸ”¥ Increase randomness\n",
    "            top_k=40,  # ðŸ”¥ Smaller k for more diversity\n",
    "            top_p=0.8,  # ðŸ”¥ Smaller p for controlled randomness\n",
    "            repetition_penalty=1.2,  # ðŸ”¥ Reduce repetition\n",
    "            pad_token_id=tokenizer.eos_token_id  \n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Recipe:\n",
      " Title: pizza\n",
      "Servings: 2\n",
      "Ingredients: [\"1 (8 oz.) pkg. cream cheese\", \"2 c. chopped onion\", \"(optional) shredded Cheddar Cheese\"]\n",
      "Directions; [\"Mix all ingredients together.\", \"-Chill.\"] Yield: 4 servings.\"]\n",
      "Directions: [Makes 6 to 8 pizzas, each with 1/4 cup of the topping and a small amount of toppiness for dipping.\"]\n",
      "Ingredients(in order): [\"3 Tbsp. butter or margarine softened\", \". Spread on top of crust.\", \",Bake at 350\\u00b0 until done.\"]\n",
      "Directions: \"[Dip in melted butter if desired but do not spread over filling as it will be too thick.\"] Makes about 3 dozen pies.\"] Recipe adapted from The New York Times Best-Selling Book by Tom Stoppard.\"]\n",
      "Ingredients: [\"6 slices bacon, cut into bite-size pieces - sliced thin so they don't stick out like paper cutouts.\", \"\\tSpread remaining crumbs around bottom of pie shell then sprinkle with additional crumb topping.\", \\nAdd more Crumb topping after you have filled up your pan enough to cover both sides of the crust.\", \\\"\\\"If using an electric mixer add some milk before mixing well!\", \"'\\\"You can also use a fork instead of a spoon!\"].\"] Ingredients used: [\"5 eggs\", \"/salt and pepper to taste\", /sugar\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Title: pizza\\nServings: 2\\nIngredients:\"\n",
    "generated_recipe = generate_recipe(prompt)\n",
    "\n",
    "print(\"Generated Recipe:\\n\", generated_recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Recipe:\n",
      " Title: Pizza\n",
      "Servings: 2\n",
      "Ingredients:\n",
      "1.\n",
      "2 c. flour\", \"3/4 tsp salt\"] [\"6 Tbsp. butter, melted\", \"(optional) pepperoni slices (8 oz.) or 12 lb.\", \", cut into 1 inch pieces and bake at 350\\u00b0 for 30 minutes.\"]\n",
      "\n",
      "*Note: I use a combination of margarine with my pizza dough to make this crust as it is easier to work on the edges than in the center. It's not necessary to add any more water because you will be making your own filling instead!\"], \"[I also like using unsweetened cocoa powder so that there is no fat added during baking time - if needed).\"; \". .\",\"\", \"-The last 5 ingredients are optional but very important -- they allow me freedom from the needlessly long process which makes pie rolls look too thin when baked--and even better! They help prevent yeast infection!\",\"....\"\"This recipe yields about\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = r\"D:\\yamuna\\practise_file\\fine_tuned_recipe_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Ensure padding is set correctly\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to generate a recipe\n",
    "def generate_recipe(dish_name, servings, max_length=200):\n",
    "    prompt = \"\"\"Title: {dish_name}\n",
    "                Servings: {servings}\n",
    "                Ingredients:\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],  \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            temperature=0.6,  \n",
    "            top_k=40, \n",
    "            top_p=0.85, \n",
    "            repetition_penalty=1.7,\n",
    "            do_sample=True,  \n",
    "            pad_token_id=tokenizer.eos_token_id  \n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 124,439,808\n",
      "Trainable Parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Load your fine-tuned model\n",
    "model = AutoModel.from_pretrained(r\"D:\\yamuna\\Kitchen_IQ\\fine_tuned_recipe_gpt2\")\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Count trainable parameters (if fine-tuning)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
